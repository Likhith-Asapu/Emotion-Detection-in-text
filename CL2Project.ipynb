{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CL2Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJVQIQ1V8NPG"
      },
      "source": [
        "# Importing Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKtsjpRFAtVF"
      },
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "import sklearn.model_selection as model_selection"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBw2nK2wDZsj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns; sns.set(font_scale = 1.2)\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RnmIb6QeAd3E",
        "outputId": "8170cdee-a0a7-4ce3-edb4-5e05813805b6"
      },
      "source": [
        "import json\n",
        "f = open('emojiclassify.json')\n",
        "emojidata = json.load(f)\n",
        "emojidata[\"emojis\"]['üò†']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'anger'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NcIIZ5LUVsay",
        "outputId": "41ee608a-d66c-4c19-e7b8-b7850f9656db"
      },
      "source": [
        "file = open(\"CodeMixedTestData.txt\",\"r\")\n",
        "data = file.readlines()\n",
        "data[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<tweet>\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNQIrl0PVsxy"
      },
      "source": [
        "tweets = []\n",
        "for line in data:\n",
        "  line = line.split()\n",
        "  if len(line) > 0:\n",
        "    if line[0] == '<tweet>':\n",
        "      readtweet = True\n",
        "      tweet = []\n",
        "      continue\n",
        "    elif line[0] == '</tweet>':\n",
        "      readtweet = False\n",
        "\n",
        "    if readtweet == True:\n",
        "      if line[0] == \"<emotion>\" :\n",
        "        tweets.append({'text':tweet,\"emotion\":line[1]})\n",
        "      else:\n",
        "        tweet.append({'word':line[0],'lang':line[1]})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfC7J4Qy8b9s"
      },
      "source": [
        "# Emoji and Emoticon Feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDa-gypqF7ux",
        "outputId": "e4c1767a-e9d1-4462-8d2c-d877bd9c6197"
      },
      "source": [
        "pip install emoji"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà                              | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñâ                            | 20 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 30 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 40 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 81 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 133 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 143 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 153 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 163 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 7.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=bd14b086a5d0106629ceb77a2227e2d03b2201c83494931d8700ad9853be13ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXFbAvpFf3n"
      },
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tFceL2HF5wv"
      },
      "source": [
        "all_features = []\n",
        "\n",
        "def emoji_features(text):\n",
        "  feature = [0,0,0]\n",
        "  emojis_found = extract_emojis(text)\n",
        "  for c in emojis_found:\n",
        "    try:\n",
        "      if emojidata[\"emojis\"][c] == \"happiness\":\n",
        "        feature[0] = feature[0] + 1\n",
        "      elif emojidata[\"emojis\"][c] == \"sadness\":\n",
        "        feature[1] = feature[1] + 1\n",
        "      elif emojidata[\"emojis\"][c] == \"anger\":\n",
        "        feature[2] = feature[2] + 1\n",
        "    except:\n",
        "      pass\n",
        "  return feature"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCOg-4P_8olX"
      },
      "source": [
        "# Punctuation Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjiTQeEiqf4x"
      },
      "source": [
        "def punctuation_count(text):\n",
        "  punctuation_num = 0\n",
        "  for c in text:\n",
        "    if c in ['!','?']:\n",
        "      punctuation_num =  punctuation_num + 1\n",
        "\n",
        "  return  punctuation_num"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EY9yn8BMdQ"
      },
      "source": [
        "# Creating feature vector \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQxwXxM1o8MP",
        "outputId": "1c764595-184f-4351-ef69-e00c0c3cef6e"
      },
      "source": [
        "for tweet in tweets:\n",
        "  othertext = ''\n",
        "  for word in tweet['text']:\n",
        "    if word['lang'] == 'OTH':\n",
        "      othertext = othertext + word['word']\n",
        "\n",
        "  feature = emoji_features(othertext)\n",
        "  punctuation_num = punctuation_count(othertext)\n",
        "  feature.append(punctuation_num)\n",
        "  all_features.append(feature)\n",
        "\n",
        "print(all_features)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [2, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 2], [0, 1, 0, 0], [0, 2, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 3, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKtqwossX1QJ"
      },
      "source": [
        "# Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ6akqkbYxno"
      },
      "source": [
        "import re"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sDNBO-Fa6rc",
        "outputId": "8ac9667c-3064-4148-a064-4138a9a9b589"
      },
      "source": [
        "tweetnum = 0\n",
        "tweettextonly = []\n",
        "for tweet in tweets:\n",
        "  textonly = ''\n",
        "  removewordlist = []\n",
        "  for word in tweet['text']:\n",
        "    if word['lang'] == 'OTH':\n",
        "      removewordlist.append(word)\n",
        "  for word in removewordlist:\n",
        "    tweet['text'].remove(word)\n",
        "  for word in tweet['text']:\n",
        "    textonly = textonly + ' ' + word['word']\n",
        "  tweettextonly.append(textonly)\n",
        "print(tweettextonly)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' I am happy loooool', ' I LOVE YOU', ' I am happy our mai khush hu', ' I am very happy', ' I am happy', ' I am happy', ' I am happy', ' I am happy', ' I am happy', ' I am Sad', ' I am Sad', ' I am Sad', ' I am Sad', ' I am angry', ' I AM ANGRY', ' I AM ANGRY', ' I AM ANGRY']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kL7IE9blsiH",
        "outputId": "8900da04-e283-4b66-ba60-3489b8044d87"
      },
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.11.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 15.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17415 sha256=53ae0f558319e763bff180575083a0f08b1a2624b417061a2f1a1bce99766d05\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/00/4fe71786ea6d12314b29037620c36d857e5d104ac2748bf82a\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fhQcHsylmBF2",
        "outputId": "978df667-8e10-4b02-b9fd-449cdf55a23e"
      },
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "translator.translate(\"mai khush hu\",dest='hi').text"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‡§Æ‡•à ‡§ñ‡•Å‡§∂ ‡§π‡•Å'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2336M4IalR0S",
        "outputId": "13f9a6b9-a005-4052-9bb7-381580930bbb"
      },
      "source": [
        "for tweet in tweets:\n",
        "  hintext = ''\n",
        "  for word in tweet['text']:\n",
        "    if word['lang'] == 'HIN':\n",
        "      print(word['word'])\n",
        "      hintext = hintext + ' ' + word['word']\n",
        "  if hintext != '':\n",
        "    devtext = translator.translate(hintext,dest='hi').text\n",
        "    devtext = devtext.split()\n",
        "    wordnum = 0\n",
        "    for word in tweet['text']:\n",
        "      if word['lang'] == 'HIN':\n",
        "        word['word'] = devtext[wordnum]\n",
        "        wordnum = wordnum + 1\n",
        "tweets"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our\n",
            "mai\n",
            "khush\n",
            "hu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'},\n",
              "   {'lang': 'ENG', 'word': 'loooool'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'LOVE'},\n",
              "   {'lang': 'ENG', 'word': 'YOU'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ü‡§µ‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§ñ‡•Å‡§∂'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•Å'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'very'},\n",
              "   {'lang': 'ENG', 'word': 'happy'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'happy'}]},\n",
              " {'emotion': 'Sadness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'Sad'}]},\n",
              " {'emotion': 'Sadness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'Sad'}]},\n",
              " {'emotion': 'Sadness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'Sad'}]},\n",
              " {'emotion': 'Sadness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'Sad'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'am'},\n",
              "   {'lang': 'ENG', 'word': 'angry'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'AM'},\n",
              "   {'lang': 'ENG', 'word': 'ANGRY'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'AM'},\n",
              "   {'lang': 'ENG', 'word': 'ANGRY'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'ENG', 'word': 'I'},\n",
              "   {'lang': 'ENG', 'word': 'AM'},\n",
              "   {'lang': 'ENG', 'word': 'ANGRY'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlkb8Vf4lSOt"
      },
      "source": [
        "# Uppercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xazEyF9a6yg"
      },
      "source": [
        "def uppercase_feature(text):\n",
        "  splittweet = text.split()\n",
        "  uppercase_words = 0\n",
        "  for word in splittweet:   \n",
        "    if word.upper() == word and len(word) > 1:\n",
        "      uppercase_words = uppercase_words + 1\n",
        "  return uppercase_words"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4s3ThqOKRE4",
        "outputId": "57cb6042-4e82-44d3-e1fc-1e126190b3bf"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  uppercase_words = uppercase_feature(tweet)\n",
        "  all_features[tweetnum].append(uppercase_words)\n",
        "  tweetnum = tweetnum + 1\n",
        "  \n",
        "print(all_features)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0, 0], [1, 0, 0, 0, 2], [1, 0, 0, 0, 0], [2, 0, 0, 0, 0], [2, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 2, 0], [0, 1, 0, 0, 0], [0, 2, 0, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 3, 0, 0], [0, 0, 1, 0, 2], [0, 0, 1, 0, 2], [0, 0, 1, 0, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyzVeFIaRqWl"
      },
      "source": [
        "# Repeated Charater Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJGG0M2ZIQD0"
      },
      "source": [
        "def repeated_character_feature(text):\n",
        "  splittweet = text.split()\n",
        "  repeated_words = 0\n",
        "  for word in splittweet:   \n",
        "    first_word = ''\n",
        "    second_word = ''\n",
        "    for c in word:\n",
        "      if c == first_word and first_word == second_word:\n",
        "        repeated_words = repeated_words + 1\n",
        "        break\n",
        "      first_word = second_word\n",
        "      second_word = c\n",
        "  return repeated_words"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkYEcVy0K8l_",
        "outputId": "46d67312-ec08-4cc1-d720-5fcc052c1d87"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  repeated_words = repeated_character_feature(tweet)\n",
        "  all_features[tweetnum].append(repeated_words)\n",
        "  tweetnum = tweetnum + 1\n",
        "\n",
        "print(all_features)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 2, 0], [1, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 2, 0, 0], [0, 1, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0], [0, 0, 1, 0, 2, 0], [0, 0, 1, 0, 2, 0], [0, 0, 1, 0, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGzOkoLR2zN"
      },
      "source": [
        "# Negation Words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIU48ozpX4Kc",
        "outputId": "e62e75b5-220a-46f6-f097-4854198acd10"
      },
      "source": [
        "# importing english negated words\n",
        "file = open(\"Negationwords.txt\",\"r\")\n",
        "NegatedWords = file.read()\n",
        "NegatedWordsList = NegatedWords.split()\n",
        "NegatedWordsList"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['never',\n",
              " 'no',\n",
              " 'nothing',\n",
              " 'nowhere',\n",
              " 'noone',\n",
              " 'none',\n",
              " 'not',\n",
              " 'havent',\n",
              " 'hasnt',\n",
              " 'hadnt',\n",
              " 'cant',\n",
              " 'couldnt',\n",
              " 'shouldnt',\n",
              " 'wont',\n",
              " 'wouldnt',\n",
              " 'dont',\n",
              " 'doesnt',\n",
              " 'didnt',\n",
              " 'isnt',\n",
              " 'arent',\n",
              " 'aint',\n",
              " 'nahi',\n",
              " 'nhi']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GVipENER2b5"
      },
      "source": [
        "def negatedwords_feature(text):\n",
        "  splittweet = text.split()\n",
        "  negated_words = 0\n",
        "  for word in splittweet:   \n",
        "    if word.lower() in NegatedWordsList:\n",
        "      negated_words = negated_words + 1\n",
        "  return negated_words"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSoSGIV3X11s",
        "outputId": "66355645-0bdd-40e3-abe6-ec2e92733a10"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  negated_words = negatedwords_feature(tweet)\n",
        "  all_features[tweetnum].append(negated_words)\n",
        "  tweetnum = tweetnum + 1\n",
        "\n",
        "print(all_features)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 2, 0, 0], [1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 2, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0], [0, 0, 1, 0, 2, 0, 0], [0, 0, 1, 0, 2, 0, 0], [0, 0, 1, 0, 2, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhiiCtV61COK"
      },
      "source": [
        "# Lexicon Emotion Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "hNKQvmmu1TVg",
        "outputId": "7c1d9d0f-a2de-4fa9-e32c-b65169a66a6f"
      },
      "source": [
        "englexicon = pd.read_excel('NRC-Emotion-Lexicon.xlsx', sheet_name=None)\n",
        "englexicon = englexicon['NRC-Lex-v0.92-word-translations']\n",
        "englexicon.set_index(\"English (en)\", inplace = True)\n",
        "englexicon"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hindi (hi)</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Anger</th>\n",
              "      <th>Disgust</th>\n",
              "      <th>Fear</th>\n",
              "      <th>Joy</th>\n",
              "      <th>Sadness</th>\n",
              "      <th>Surprise</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>English (en)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aback</th>\n",
              "      <td>‡§Ö‡§ö‡§Ç‡§≠‡•á</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abacus</th>\n",
              "      <td>‡§Ö‡§¨‡•á‡§ï‡§∏</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandon</th>\n",
              "      <td>‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§®‡§æ</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandoned</th>\n",
              "      <td>‡§§‡•ç‡§Ø‡§æ‡§ó‡§æ ‡§π‡•Å‡§Ü</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandonment</th>\n",
              "      <td>‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zone</th>\n",
              "      <td>‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoo</th>\n",
              "      <td>‡§ö‡§ø‡§°‡§º‡§ø‡§Ø‡§æ‡§ò‡§∞</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoological</th>\n",
              "      <td>‡§™‡•ç‡§∞‡§æ‡§£‡§ø</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoology</th>\n",
              "      <td>‡§™‡•ç‡§∞‡§æ‡§£‡§ø ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§®</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoom</th>\n",
              "      <td>‡§ú‡§º‡•Ç‡§Æ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14182 rows √ó 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Hindi (hi)  Positive  Negative  ...  Joy  Sadness  Surprise\n",
              "English (en)                                      ...                        \n",
              "aback                  ‡§Ö‡§ö‡§Ç‡§≠‡•á         0         0  ...    0        0         0\n",
              "abacus                 ‡§Ö‡§¨‡•á‡§ï‡§∏         0         0  ...    0        0         0\n",
              "abandon            ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§®‡§æ         0         1  ...    0        1         0\n",
              "abandoned         ‡§§‡•ç‡§Ø‡§æ‡§ó‡§æ ‡§π‡•Å‡§Ü         0         1  ...    0        1         0\n",
              "abandonment          ‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏         0         1  ...    0        1         1\n",
              "...                      ...       ...       ...  ...  ...      ...       ...\n",
              "zone                 ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞         0         0  ...    0        0         0\n",
              "zoo                ‡§ö‡§ø‡§°‡§º‡§ø‡§Ø‡§æ‡§ò‡§∞         0         0  ...    0        0         0\n",
              "zoological            ‡§™‡•ç‡§∞‡§æ‡§£‡§ø         0         0  ...    0        0         0\n",
              "zoology       ‡§™‡•ç‡§∞‡§æ‡§£‡§ø ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§®         0         0  ...    0        0         0\n",
              "zoom                    ‡§ú‡§º‡•Ç‡§Æ         0         0  ...    0        0         0\n",
              "\n",
              "[14182 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcYAcydx9ZDW"
      },
      "source": [
        "hinlexicon = pd.read_excel('NRC-Emotion-LexiconHindi.xlsx', sheet_name=None)\n",
        "hinlexicon = hinlexicon['NRC-Lex-v0.92-word-translations']\n",
        "hinlexicon.set_index(\"Hindi (hi)\",inplace = True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTbOT0Fh1JZA"
      },
      "source": [
        "def lexicon_emotion_feature(text):\n",
        "  tweetemotion = [0,0,0]\n",
        "  for word in text['text']:\n",
        "    try:\n",
        "      if word['lang'] == 'ENG':\n",
        "        wordemotion = englexicon.loc[word['word'].lower()]\n",
        "      elif word['lang'] == 'HIN':\n",
        "        wordemotion = hinlexicon.loc[word['word'].lower()]\n",
        "      tweetemotion[0] = tweetemotion[0] + wordemotion['Joy']\n",
        "      tweetemotion[1] = tweetemotion[1] + wordemotion['Sadness']\n",
        "      tweetemotion[2] = tweetemotion[2] + wordemotion['Anger']\n",
        "    except:\n",
        "      pass\n",
        "  return tweetemotion"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQlAwsXoZa6X",
        "outputId": "5a0432eb-41fb-4a0a-d0eb-9da594b86e4d"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweets:\n",
        "  tweetemotion = lexicon_emotion_feature(tweet)\n",
        "  all_features[tweetnum] = all_features[tweetnum] + tweetemotion\n",
        "  tweetnum = tweetnum + 1\n",
        "\n",
        "print(all_features)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 2, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 2, 0, 0], [2, 0, 0, 0, 0, 0, 0, 1, 0, 0], [2, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 2, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 2, 0, 0, 0, 0, 1], [0, 0, 1, 0, 2, 0, 0, 0, 0, 1], [0, 0, 1, 0, 2, 0, 0, 0, 0, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTSSFLnFtGZ"
      },
      "source": [
        "# Intensifier Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xppKwylzFxxh"
      },
      "source": [
        "# importing english intensifiers\n",
        "file = open(\"IntensifierList.txt\",\"r\")\n",
        "Intensifiers = file.read()\n",
        "IntensifiersList = Intensifiers.split()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC7EtcDwa0Ri"
      },
      "source": [
        "def Intensifiers_feature(text):\n",
        "  splittweet = text.split()\n",
        "  Intensifier_words = 0\n",
        "  for word in splittweet:   \n",
        "    if word.lower() in IntensifiersList:\n",
        "      Intensifier_words = Intensifier_words + 1\n",
        "  return Intensifier_words"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJKBI6hmZ0Gl",
        "outputId": "adca8d3d-d96d-4753-bac8-51c57f12e37d"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  Intensifier_words = Intensifiers_feature(tweet)\n",
        "  all_features[tweetnum].append(Intensifier_words)\n",
        "  tweetnum = tweetnum + 1\n",
        "\n",
        "print(all_features)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om3dM52t9eJU"
      },
      "source": [
        "# Text Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKYsER11GRpe",
        "outputId": "bb2fa017-fcf3-4551-ae19-1cebc4547115"
      },
      "source": [
        "tag = []\n",
        "for tweet in tweets:\n",
        "  tag.append(tweet['emotion'])\n",
        "print(tag)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Happiness', 'Happiness', 'Happiness', 'Happiness', 'Happiness', 'Happiness', 'Happiness', 'Happiness', 'Happiness', 'Sadness', 'Sadness', 'Sadness', 'Sadness', 'Anger', 'Anger', 'Anger', 'Anger']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y368tcSG9qtN"
      },
      "source": [
        "# Tagging and Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkLejtQ5JnI0"
      },
      "source": [
        "X = all_features\n",
        "y = tag\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1).fit(X, y)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzrb2XaDJrJW",
        "outputId": "d11b31a3-ca14-42b2-fe89-99b8404f7077"
      },
      "source": [
        "poly_pred = poly.predict([[0, 5, 0, 0, 0, 1, 0, 1, 0, 0, 0]])\n",
        "print(poly_pred)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sadness']\n"
          ]
        }
      ]
    }
  ]
}