{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CL2Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJVQIQ1V8NPG"
      },
      "source": [
        "# Importing Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKtsjpRFAtVF"
      },
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "import sklearn.model_selection as model_selection"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBw2nK2wDZsj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns; sns.set(font_scale = 1.2)\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RnmIb6QeAd3E",
        "outputId": "90e38edc-0c5f-4403-b3df-c0e2dcb7c2f3"
      },
      "source": [
        "import json\n",
        "f = open('emojiclassify.json')\n",
        "emojidata = json.load(f)\n",
        "emojidata[\"emojis\"]['üò†']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'anger'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NcIIZ5LUVsay",
        "outputId": "fb2193c0-434c-4430-abdc-2dfa33d21d68"
      },
      "source": [
        "file = open(\"LikhithFinishedTags.txt\",\"r\")\n",
        "data = file.readlines()\n",
        "data[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<tweet>\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNQIrl0PVsxy"
      },
      "source": [
        "tweets = []\n",
        "for line in data:\n",
        "  line = line.split()\n",
        "  if len(line) > 0:\n",
        "    if line[0] == '<tweet>':\n",
        "      readtweet = True\n",
        "      tweet = []\n",
        "      continue\n",
        "    elif line[0] == '</tweet>':\n",
        "      readtweet = False\n",
        "\n",
        "    if readtweet == True:\n",
        "      if line[0] == \"<emotion>\" :\n",
        "        tweets.append({'text':tweet,\"emotion\":line[1]})\n",
        "      else:\n",
        "        try:\n",
        "          tweet.append({'word':line[0],'lang':line[1]})\n",
        "        except:\n",
        "          pass"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfC7J4Qy8b9s"
      },
      "source": [
        "# Emoji and Emoticon Feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDa-gypqF7ux",
        "outputId": "eff27db7-df51-4e66-b013-a889db9fb7ad"
      },
      "source": [
        "pip install emoji"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXFbAvpFf3n"
      },
      "source": [
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tFceL2HF5wv"
      },
      "source": [
        "all_features = []\n",
        "\n",
        "def emoji_features(text):\n",
        "  feature = [0,0,0]\n",
        "  emojis_found = extract_emojis(text)\n",
        "  for c in emojis_found:\n",
        "    try:\n",
        "      if emojidata[\"emojis\"][c] == \"happiness\":\n",
        "        feature[0] = feature[0] + 1\n",
        "      elif emojidata[\"emojis\"][c] == \"sadness\":\n",
        "        feature[1] = feature[1] + 1\n",
        "      elif emojidata[\"emojis\"][c] == \"anger\":\n",
        "        feature[2] = feature[2] + 1\n",
        "    except:\n",
        "      pass\n",
        "  return feature"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCOg-4P_8olX"
      },
      "source": [
        "# Punctuation Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjiTQeEiqf4x"
      },
      "source": [
        "def punctuation_count(text):\n",
        "  punctuation_num = 0\n",
        "  for c in text:\n",
        "    if c in ['!','?']:\n",
        "      punctuation_num =  punctuation_num + 1\n",
        "\n",
        "  return  punctuation_num"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EY9yn8BMdQ"
      },
      "source": [
        "# Creating feature vector \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQxwXxM1o8MP"
      },
      "source": [
        "for tweet in tweets:\n",
        "  othertext = ''\n",
        "  for word in tweet['text']:\n",
        "    if word['lang'] == 'OTH':\n",
        "      othertext = othertext + word['word']\n",
        "\n",
        "  feature = emoji_features(othertext)\n",
        "  punctuation_num = punctuation_count(othertext)\n",
        "  feature.append(punctuation_num)\n",
        "  all_features.append(feature)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKtqwossX1QJ"
      },
      "source": [
        "# Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ6akqkbYxno"
      },
      "source": [
        "import re"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sDNBO-Fa6rc"
      },
      "source": [
        "tweetnum = 0\n",
        "tweettextonly = []\n",
        "for tweet in tweets:\n",
        "  textonly = ''\n",
        "  removewordlist = []\n",
        "  for word in tweet['text']:\n",
        "    if word['lang'] == 'OTH':\n",
        "      removewordlist.append(word)\n",
        "  for word in removewordlist:\n",
        "    tweet['text'].remove(word)\n",
        "  for word in tweet['text']:\n",
        "    textonly = textonly + ' ' + word['word']\n",
        "  tweettextonly.append(textonly)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kL7IE9blsiH",
        "outputId": "77f1b438-47c4-4347-9d9a-229698f47b96"
      },
      "source": [
        " pip install git+https://github.com/siddharth17196/english-hindi-transliteration"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/siddharth17196/english-hindi-transliteration\n",
            "  Cloning https://github.com/siddharth17196/english-hindi-transliteration to /tmp/pip-req-build-1sat5_h2\n",
            "  Running command git clone -q https://github.com/siddharth17196/english-hindi-transliteration /tmp/pip-req-build-1sat5_h2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhQcHsylmBF2",
        "outputId": "45d5e5cc-7b47-4d88-813c-3be8772ae393"
      },
      "source": [
        "from elt import translit\n",
        "to_hindi = translit('hindi')\n",
        "to_hindi.convert(['1'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‡•ß']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2336M4IalR0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9acbb0-a8da-4e0a-b601-0227553f89c3"
      },
      "source": [
        "for tweet in tweets:\n",
        "  hintext = ''\n",
        "  for word in tweet['text']:\n",
        "    if word['lang'] == 'HIN':\n",
        "      try:\n",
        "        newword = to_hindi.convert([word['word']])\n",
        "        word['word'] = newword[0]\n",
        "      except:\n",
        "        pass\n",
        "tweets[0:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'emotion': 'Anger',\n",
              "  'text': [{'lang': 'ENG', 'word': 'ban'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•ã‡§®‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§ö‡§æ‡§π‡§ø‡§è'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§è‡§ï'},\n",
              "   {'lang': 'HIN', 'word': '‡§á‡§ö‡§õ'},\n",
              "   {'lang': 'ENG', 'word': 'trophy'},\n",
              "   {'lang': 'HIN', 'word': '‡§®‡§π‡•Ä‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§ú‡•Ä‡§§‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'under'},\n",
              "   {'lang': 'ENG', 'word': 'his'},\n",
              "   {'lang': 'ENG', 'word': 'captaincy'},\n",
              "   {'lang': 'HIN', 'word': '‡§î‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä.'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§æ‡§∞‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§§‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§æ‡§∞‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§â‡§™‡•ç‡§™‡•á‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'attitude'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡§π‡•ã.'},\n",
              "   {'lang': 'HIN', 'word': '‡§¶‡•á‡§∂'},\n",
              "   {'lang': 'ENG', 'word': 'ko'},\n",
              "   {'lang': 'ENG', 'word': 'pal'},\n",
              "   {'lang': 'HIN', 'word': '‡§∞‡§π‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§ê‡§∏‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡§ó‡§§‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à.'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§ï‡•Å‡§õ'},\n",
              "   {'lang': 'ENG', 'word': 'action'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡•á‡§®‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§ö‡§æ‡§π‡§ø‡§è'}]},\n",
              " {'emotion': 'Sadness',\n",
              "  'text': [{'lang': 'ENG', 'word': 'If'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡§ø‡§è'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡•ã‡§≤'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§¶‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§§‡•ã‡§π'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•Å‡§õ'},\n",
              "   {'lang': 'HIN', 'word': '‡§®‡§π‡•Ä‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•ã‡§®‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•á‡§Ç'},\n",
              "   {'lang': 'ENG', 'word': 'unsold'},\n",
              "   {'lang': 'HIN', 'word': '‡§•‡§æ'},\n",
              "   {'lang': 'ENG', 'word': 'go'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•á‡§Ç'},\n",
              "   {'lang': 'ENG', 'word': 'mi'},\n",
              "   {'lang': 'ENG', 'word': 'ne'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡§ø‡§Ø‡§æ'},\n",
              "   {'lang': 'ENG', 'word': 'matches'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•á‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§´‡§ø‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§®‡§π‡•Ä‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§ñ‡§ø‡§≤‡§æ‡§Ø‡§æ'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'ENG', 'word': 'road'},\n",
              "   {'lang': 'ENG', 'word': 'safety'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡§ø‡§è'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§æ‡§á‡§™'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡§ø‡§≤‡§ï‡•Å‡§≤'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡§ï‡§µ‡§æ‡§∏'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡§¨'},\n",
              "   {'lang': 'HIN', 'word': '‡§ñ‡•á‡§≤‡§§‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§ï‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡§ø‡§è'},\n",
              "   {'lang': 'HIN', 'word': '‡§µ‡§æ‡§™‡§∏'},\n",
              "   {'lang': 'HIN', 'word': '‡§ñ‡•á‡§≤‡§®‡§æ'},\n",
              "   {'lang': 'ENG', 'word': 'useless'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡•á‡§ï‡§æ‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§æ‡§á‡§™'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§Æ‡•Å‡§ù‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ü‡§™‡§∏‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡•ú‡§®‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'ka'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡•Å‡§ñ'},\n",
              "   {'lang': 'HIN', 'word': '‡§®‡§π‡•Ä‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡§à'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡§∏'},\n",
              "   {'lang': 'HIN', 'word': '‡§á‡§§‡§®‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡§π‡§æ‡§Å'},\n",
              "   {'lang': 'HIN', 'word': '‡§ö‡§æ‡§π‡§§‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•Å'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ü‡§à‡§™‡•Ä‡§è‡§≤'},\n",
              "   {'lang': 'HIN', 'word': '‡§á‡§∏'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡§æ‡§≤'},\n",
              "   {'lang': 'HIN', 'word': '‡§®‡§π‡•Ä‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ü‡§Ø‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'ENG', 'word': 'wo'},\n",
              "   {'lang': 'HIN', 'word': '‡§™‡§ø‡§õ‡§≤‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'Year'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§∞‡§π‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§î‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§™‡§ø‡§õ‡§≤‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'Year'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§∞'},\n",
              "   {'lang': 'ENG', 'word': 'tournament'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•á‡§Ç'},\n",
              "   {'lang': 'ENG', 'word': 'Final'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ø‡§æ'},\n",
              "   {'lang': 'ENG', 'word': 'Semifinal'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•á‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§∞‡§π‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§§‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§è‡§ï'},\n",
              "   {'lang': 'HIN', 'word': '‡•ô‡§∞‡§æ‡§¨'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§®'},\n",
              "   {'lang': 'ENG', 'word': 'Team'},\n",
              "   {'lang': 'HIN', 'word': '‡•ô‡§∞‡§æ‡§¨'},\n",
              "   {'lang': 'ENG', 'word': 'hoti'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§î‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§®'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡§æ'},\n",
              "   {'lang': 'ENG', 'word': 'standard'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§¨‡•ã‡§≤‡§§‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§§‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•Å'},\n",
              "   {'lang': 'ENG', 'word': 'ko'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä‡§ñ'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡§§'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡§æ‡§®‡§ó‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§Æ‡§∏‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§â‡§ß‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡•á‡§≤‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡§ó‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§´‡§ø‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'pass'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä‡§ï'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡§æ‡§Ç‡§ó‡§§‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§´‡§ø‡§∞‡§§‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§î‡§∞'},\n",
              "   {'lang': 'HIN', 'word': '‡§è‡§ï'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§Æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'HIN', 'word': '‡§ú‡•ã'},\n",
              "   {'lang': 'ENG', 'word': 'billion'},\n",
              "   {'lang': 'ENG', 'word': 'dollars'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§¶‡•ã'},\n",
              "   {'lang': 'ENG', 'word': 'team'},\n",
              "   {'lang': 'HIN', 'word': '‡§Æ‡•á‡§Ç'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡•á‡§ö'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§¨‡•à‡§†‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'}]},\n",
              " {'emotion': 'Anger',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§≠‡§æ‡§à'},\n",
              "   {'lang': 'ENG', 'word': 'time'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à?'},\n",
              "   {'lang': 'HIN', 'word': '‡§è‡§ï'},\n",
              "   {'lang': 'HIN', 'word': '‡§¶‡§ø‡§®'},\n",
              "   {'lang': 'HIN', 'word': '‡§¶‡•á‡§ñ‡§ø‡§≤‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡§ø‡§∞‡•ç‡§´'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•ã‡§ó‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ö‡§≠‡•Ä'},\n",
              "   {'lang': 'HIN', 'word': '‡§∏‡•á'},\n",
              "   {'lang': 'ENG', 'word': 'team'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡§æ'},\n",
              "   {'lang': 'ENG', 'word': 'interest'},\n",
              "   {'lang': 'HIN', 'word': '‡•ô‡§§‡§Æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§¶‡§ø‡§ñ'},\n",
              "   {'lang': 'HIN', 'word': '‡§∞‡§π‡§æ'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'}]},\n",
              " {'emotion': 'Happiness',\n",
              "  'text': [{'lang': 'HIN', 'word': '‡§Ü‡§™‡§ï‡•ã'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡•Ä'},\n",
              "   {'lang': 'ENG', 'word': 'Semis'},\n",
              "   {'lang': 'HIN', 'word': '‡§ï‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§≤‡§ø‡§è'},\n",
              "   {'lang': 'HIN', 'word': '‡§≠‡§æ‡§à'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡§Æ'},\n",
              "   {'lang': 'ENG', 'word': 'to'},\n",
              "   {'lang': 'ENG', 'word': 'aa'},\n",
              "   {'lang': 'HIN', 'word': '‡§∞‡§π‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§π‡•à'},\n",
              "   {'lang': 'ENG', 'word': 'return'},\n",
              "   {'lang': 'HIN', 'word': '‡§ñ‡•á‡§≤‡•á‡§Ç‡§ó‡•á'},\n",
              "   {'lang': 'HIN', 'word': '‡§Ö‡§¨'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlkb8Vf4lSOt"
      },
      "source": [
        "# Uppercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xazEyF9a6yg"
      },
      "source": [
        "def uppercase_feature(text):\n",
        "  splittweet = text.split()\n",
        "  uppercase_words = 0\n",
        "  for word in splittweet:   \n",
        "    if word.upper() == word and len(word) > 1:\n",
        "      uppercase_words = uppercase_words + 1\n",
        "  return uppercase_words"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4s3ThqOKRE4"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  uppercase_words = uppercase_feature(tweet)\n",
        "  all_features[tweetnum].append(uppercase_words)\n",
        "  tweetnum = tweetnum + 1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyzVeFIaRqWl"
      },
      "source": [
        "# Repeated Charater Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJGG0M2ZIQD0"
      },
      "source": [
        "def repeated_character_feature(text):\n",
        "  splittweet = text.split()\n",
        "  repeated_words = 0\n",
        "  for word in splittweet:   \n",
        "    first_word = ''\n",
        "    second_word = ''\n",
        "    for c in word:\n",
        "      if c == first_word and first_word == second_word:\n",
        "        repeated_words = repeated_words + 1\n",
        "        break\n",
        "      first_word = second_word\n",
        "      second_word = c\n",
        "  return repeated_words"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkYEcVy0K8l_"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  repeated_words = repeated_character_feature(tweet)\n",
        "  all_features[tweetnum].append(repeated_words)\n",
        "  tweetnum = tweetnum + 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGzOkoLR2zN"
      },
      "source": [
        "# Negation Words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIU48ozpX4Kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c03205-7ef9-43c5-cbfd-03eba20411ee"
      },
      "source": [
        "# importing english negated words\n",
        "file = open(\"Negationwords.txt\",\"r\")\n",
        "NegatedWords = file.read()\n",
        "NegatedWordsList = NegatedWords.split()\n",
        "NegatedWordsList"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['never',\n",
              " 'no',\n",
              " 'nothing',\n",
              " 'nowhere',\n",
              " 'noone',\n",
              " 'none',\n",
              " 'not',\n",
              " 'havent',\n",
              " 'hasnt',\n",
              " 'hadnt',\n",
              " 'cant',\n",
              " 'couldnt',\n",
              " 'shouldnt',\n",
              " 'wont',\n",
              " 'wouldnt',\n",
              " 'dont',\n",
              " 'doesnt',\n",
              " 'didnt',\n",
              " 'isnt',\n",
              " 'arent',\n",
              " 'aint',\n",
              " 'nahi',\n",
              " 'nhi']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GVipENER2b5"
      },
      "source": [
        "def negatedwords_feature(text):\n",
        "  splittweet = text.split()\n",
        "  negated_words = 0\n",
        "  for word in splittweet:   \n",
        "    if word.lower() in NegatedWordsList:\n",
        "      negated_words = negated_words + 1\n",
        "  return negated_words"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSoSGIV3X11s"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  negated_words = negatedwords_feature(tweet)\n",
        "  all_features[tweetnum].append(negated_words)\n",
        "  tweetnum = tweetnum + 1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhiiCtV61COK"
      },
      "source": [
        "# Lexicon Emotion Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNKQvmmu1TVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "f9d1bbd5-ff15-46a9-fc4e-28a66dd6e9e3"
      },
      "source": [
        "englexicon = pd.read_excel('NRC-Emotion-Lexicon.xlsx', sheet_name=None)\n",
        "englexicon = englexicon['NRC-Lex-v0.92-word-translations']\n",
        "englexicon.set_index(\"English (en)\", inplace = True)\n",
        "englexicon"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hindi (hi)</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Anger</th>\n",
              "      <th>Disgust</th>\n",
              "      <th>Fear</th>\n",
              "      <th>Joy</th>\n",
              "      <th>Sadness</th>\n",
              "      <th>Surprise</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>English (en)</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aback</th>\n",
              "      <td>‡§Ö‡§ö‡§Ç‡§≠‡•á</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abacus</th>\n",
              "      <td>‡§Ö‡§¨‡•á‡§ï‡§∏</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandon</th>\n",
              "      <td>‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§®‡§æ</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandoned</th>\n",
              "      <td>‡§§‡•ç‡§Ø‡§æ‡§ó‡§æ ‡§π‡•Å‡§Ü</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandonment</th>\n",
              "      <td>‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zone</th>\n",
              "      <td>‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoo</th>\n",
              "      <td>‡§ö‡§ø‡§°‡§º‡§ø‡§Ø‡§æ‡§ò‡§∞</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoological</th>\n",
              "      <td>‡§™‡•ç‡§∞‡§æ‡§£‡§ø</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoology</th>\n",
              "      <td>‡§™‡•ç‡§∞‡§æ‡§£‡§ø ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§®</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoom</th>\n",
              "      <td>‡§ú‡§º‡•Ç‡§Æ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14183 rows √ó 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Hindi (hi)  Positive  Negative  ...  Joy  Sadness  Surprise\n",
              "English (en)                                      ...                        \n",
              "aback                  ‡§Ö‡§ö‡§Ç‡§≠‡•á         0         0  ...    0        0         0\n",
              "abacus                 ‡§Ö‡§¨‡•á‡§ï‡§∏         0         0  ...    0        0         0\n",
              "abandon            ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§®‡§æ         0         1  ...    0        1         0\n",
              "abandoned         ‡§§‡•ç‡§Ø‡§æ‡§ó‡§æ ‡§π‡•Å‡§Ü         0         1  ...    0        1         0\n",
              "abandonment          ‡§∏‡§Ç‡§®‡•ç‡§Ø‡§æ‡§∏         0         1  ...    0        1         1\n",
              "...                      ...       ...       ...  ...  ...      ...       ...\n",
              "zone                 ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞         0         0  ...    0        0         0\n",
              "zoo                ‡§ö‡§ø‡§°‡§º‡§ø‡§Ø‡§æ‡§ò‡§∞         0         0  ...    0        0         0\n",
              "zoological            ‡§™‡•ç‡§∞‡§æ‡§£‡§ø         0         0  ...    0        0         0\n",
              "zoology       ‡§™‡•ç‡§∞‡§æ‡§£‡§ø ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§®         0         0  ...    0        0         0\n",
              "zoom                    ‡§ú‡§º‡•Ç‡§Æ         0         0  ...    0        0         0\n",
              "\n",
              "[14183 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcYAcydx9ZDW"
      },
      "source": [
        "hinlexicon = pd.read_excel('NRC-Emotion-LexiconHindi.xlsx', sheet_name=None)\n",
        "hinlexicon = hinlexicon['NRC-Lex-v0.92-word-translations']\n",
        "hinlexicon.set_index(\"Hindi (hi)\",inplace = True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTbOT0Fh1JZA"
      },
      "source": [
        "def lexicon_emotion_feature(text):\n",
        "  tweetemotion = [0,0,0]\n",
        "  for word in text['text']:\n",
        "    try:\n",
        "      if word['lang'] == 'ENG':\n",
        "        wordemotion = englexicon.loc[word['word'].lower()]\n",
        "      elif word['lang'] == 'HIN':\n",
        "        wordemotion = hinlexicon.loc[word['word'].lower()]\n",
        "      tweetemotion[0] = tweetemotion[0] + wordemotion['Joy']\n",
        "      tweetemotion[1] = tweetemotion[1] + wordemotion['Sadness']\n",
        "      tweetemotion[2] = tweetemotion[2] + wordemotion['Anger']\n",
        "    except:\n",
        "      pass\n",
        "  return tweetemotion"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQlAwsXoZa6X"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweets:\n",
        "  tweetemotion = lexicon_emotion_feature(tweet)\n",
        "  all_features[tweetnum] = all_features[tweetnum] + tweetemotion\n",
        "  tweetnum = tweetnum + 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTSSFLnFtGZ"
      },
      "source": [
        "# Intensifier Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xppKwylzFxxh"
      },
      "source": [
        "# importing english intensifiers\n",
        "file = open(\"IntensifierList.txt\",\"r\")\n",
        "Intensifiers = file.read()\n",
        "IntensifiersList = Intensifiers.split()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC7EtcDwa0Ri"
      },
      "source": [
        "def Intensifiers_feature(text):\n",
        "  splittweet = text.split()\n",
        "  Intensifier_words = 0\n",
        "  for word in splittweet:   \n",
        "    if word.lower() in IntensifiersList:\n",
        "      Intensifier_words = Intensifier_words + 1\n",
        "  return Intensifier_words"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJKBI6hmZ0Gl"
      },
      "source": [
        "tweetnum = 0\n",
        "for tweet in tweettextonly:\n",
        "  Intensifier_words = Intensifiers_feature(tweet)\n",
        "  all_features[tweetnum].append(Intensifier_words)\n",
        "  tweetnum = tweetnum + 1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arqLf27yYvX"
      },
      "source": [
        "# Word n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LIgmPNCwgzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7e3aa5-805a-497c-9913-b096d828f4ea"
      },
      "source": [
        "import nltk\n",
        "hindistopwords = []\n",
        "hindistopwordfile= open(\"HindiStopWords.txt\",\"r\")\n",
        "hindistopworddata = hindistopwordfile.readlines()\n",
        "for line in hindistopworddata:\n",
        "  line = line.split()\n",
        "  hindistopwords.append(line[0])\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y0M49gxwt5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1232b8cd-4baf-44a7-a7b6-cc378058585c"
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "num = 0\n",
        "for tweet in tweettextonly:\n",
        "  tweettextonly[num] = tweet.lower()\n",
        "  num = num + 1 \n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords = stopwords + hindistopwords\n",
        "# unigrams\n",
        "cv = CountVectorizer(ngram_range=(1,1), stop_words = stopwords)\n",
        "sparsearray = cv.fit_transform(tweettextonly)\n",
        "vocab = cv.vocabulary_\n",
        "count_values = sparsearray.toarray().sum(axis=0)\n",
        "\n",
        "unigrams = []\n",
        "num = 0\n",
        "for ug_count, ug_text in sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True):\n",
        "    unigrams.append(ug_text)\n",
        "    if(num == 50):\n",
        "      break\n",
        "    num = num + 1\n",
        "print(unigrams)\n",
        "\n",
        "#bigrams\n",
        "cv = CountVectorizer(ngram_range=(2,2), stop_words = stopwords)\n",
        "sparsearray = cv.fit_transform(tweettextonly)\n",
        "vocab = cv.vocabulary_\n",
        "count_values = sparsearray.toarray().sum(axis=0)\n",
        "\n",
        "bigrams = []\n",
        "num = 0\n",
        "for bg_count, bg_text in sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True):\n",
        "    bigrams.append(bg_text)\n",
        "    if(num == 50):\n",
        "      break\n",
        "    num = num + 1\n",
        "print(bigrams)\n",
        "\n",
        "#trigrams\n",
        "cv = CountVectorizer(ngram_range=(3,3), stop_words = stopwords)\n",
        "sparsearray = cv.fit_transform(tweettextonly)\n",
        "vocab = cv.vocabulary_\n",
        "count_values = sparsearray.toarray().sum(axis=0)\n",
        "\n",
        "trigrams = []\n",
        "num = 0\n",
        "for tg_count, tg_text in sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True):\n",
        "    trigrams.append(tg_text)\n",
        "    if(num == 50):\n",
        "      break\n",
        "    num = num + 1\n",
        "\n",
        "print(trigrams)\n",
        "\n",
        "print()\n",
        "\n",
        "result = []\n",
        "for tweet in tweettextonly:\n",
        "    vec = []\n",
        "    \n",
        "    uni = []\n",
        "    bi = []\n",
        "    tri = []\n",
        "    \n",
        "    \n",
        "    tokens = tweet.split(\" \")\n",
        "    \n",
        "    # calculate the frequency of uni-gram\n",
        "    for ug in unigrams:\n",
        "        count = 0\n",
        "        if ug in tweet:\n",
        "            for token in tokens :\n",
        "                if (ug == token):\n",
        "                    count = count + 1\n",
        "        uni.append(count)\n",
        "    vec.append(uni)\n",
        "    \n",
        "    # calculate the frequency of bi-gram\n",
        "    for bg in bigrams:\n",
        "        count = 0\n",
        "        if bg in tweet:\n",
        "            for i in range(0, len(tokens)-1):\n",
        "                bigram = tokens[i] + \" \" + tokens[i+1]\n",
        "                if (bg == bigram):\n",
        "                    count = count + 1\n",
        "        bi.append(count)\n",
        "    vec.append(bi)\n",
        "    \n",
        "    # calculate the frequency of tri-gram\n",
        "    for tg in trigrams:\n",
        "        count = 0\n",
        "        if tg in tweet:\n",
        "            for i in range(0, len(tokens)-2):\n",
        "                trigram = tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2]\n",
        "                if (tg == trigram):\n",
        "                    count = count + 1\n",
        "        tri.append(count)\n",
        "    vec.append(tri)\n",
        "    \n",
        "    result.append(vec)\n",
        "\n",
        "num = 0\n",
        "for feature in all_features:\n",
        "  all_features[num] = all_features[num] + result[num][0] + result[num][1] + result[num][2]\n",
        "  num = num + 1\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sab', 'team', 'kya', 'bhai', 'tum', 'happy', 'nhi', 'mein', 'log', 'ipl', 'chahiye', 'main', 'le', 'international', 'desh', 'yeh', 'rehti', 'logo', 'liya', 'khelte', 'khelo', 'hoga', 'har', 'din', 'dil', 'dekh', 'bas', 'tournament', 'time', 'sad', 'saal', 'paise', 'nehi', 'mujhe', 'kr', 'khel', 'karo', 'hu', 'hamare', 'form', 'fir', 'baat', 'angry', 'ab', 'yar', 'waqt', 'wahi', 'vi', 'tujhe', 'toh', 'sirf']\n",
            "['yeh ghour', 'yar wahi', 'wahi khilari', 'vi nah', 'tut geya', 'tum logoko', 'tum bachy', 'time mast', 'stalk kia', 'sab thhek', 'sab paise', 'sab hero', 'rehti tl', 'pichle year', 'paise khelte', 'nah harjaye', 'match day', 'mast rehti', 'maja alag', 'logoko dekhna', 'logo bura', 'kyonki isliye', 'kuchh khaas', 'kon ha', 'kisiko parva', 'khilari sab', 'khelte desh', 'khelo beta', 'khaas rehti', 'karo sab', 'jistaraka form', 'jao jaa', 'jaa kr', 'isliye dekne', 'ipl khelo', 'international tum', 'international cricket', 'hero dil', 'harjaye jistaraka', 'ha baap', 'ghour dekh', 'geya aab', 'dil tut', 'desh kisiko', 'dekne maja', 'dekhna bekar', 'dekh kon', 'day aajkal', 'change gae', 'break le', 'beta international']\n",
            "['yeh ghour dekh', 'yar wahi khilari', 'wahi khilari sab', 'vi nah harjaye', 'tut geya aab', 'tum logoko dekhna', 'time mast rehti', 'sab paise khelte', 'sab hero dil', 'paise khelte desh', 'nah harjaye jistaraka', 'match day aajkal', 'logoko dekhna bekar', 'kyonki isliye dekne', 'kuchh khaas rehti', 'kon ha baap', 'khelte desh kisiko', 'khelo beta international', 'khaas rehti tl', 'karo sab thhek', 'jao jaa kr', 'isliye dekne maja', 'international tum bachy', 'hero dil tut', 'harjaye jistaraka form', 'ha baap jao', 'ghour dekh kon', 'geya aab tum', 'dil tut geya', 'desh kisiko parva', 'dekne maja alag', 'dekh kon ha', 'day aajkal kuchh', 'beta international tum', 'baap jao jaa', 'aajkal kuchh khaas', 'aab tum logoko', 'zaroor milnay wala', 'yr isko bahr', 'yeh tweets waqt', 'yeh kinnar bahu', 'yeh hans rahin', 'year pichle year', 'year har tournament', 'yar uski 24', 'yaqeen tweets dekhny', 'yad rkhe hmare', 'world number team', 'world cup main', 'woke ban gya', 'woh toh honge']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['kai'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr8jahdDbu0I"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om3dM52t9eJU"
      },
      "source": [
        "# Text Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKYsER11GRpe"
      },
      "source": [
        "tag = []\n",
        "for tweet in tweets:\n",
        "  tag.append(tweet['emotion'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y368tcSG9qtN"
      },
      "source": [
        "# Tagging and Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkLejtQ5JnI0"
      },
      "source": [
        "from sklearn import svm, datasets\n",
        "import sklearn.model_selection as model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "X = all_features\n",
        "y = tag\n",
        "#poly = svm.SVC(kernel='poly', degree=3, C=1).fit(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=101)\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1).fit(X_train, y_train)\n",
        "poly_pred = poly.predict(X_test)\n",
        "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
        "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
        "#print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
        "#print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHfEKCNx9mrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacb9159-b9f2-4cf4-df9e-e56279776cc6"
      },
      "source": [
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw0C_JR5P9zn"
      },
      "source": [
        "# Text Processing\n",
        "def text_processing(text):\n",
        "    re.sub(\"[^a-zA-z]\",\"\", text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def n_grams(text) :\n",
        "    result = []\n",
        "    uni = []\n",
        "    bi = []\n",
        "    tri = []\n",
        "    \n",
        "    tokens = text.split(\" \")\n",
        "    \n",
        "    # calculate the frequency of uni-gram\n",
        "    for ug in unigrams:\n",
        "        count = 0\n",
        "        if ug in text:\n",
        "            for token in tokens :\n",
        "                if (ug == token):\n",
        "                    count = count + 1\n",
        "        uni.append(count)\n",
        "    result.append(uni)\n",
        "    \n",
        "    # calculate the frequency of bi-gram\n",
        "    for bg in bigrams:\n",
        "        count = 0\n",
        "        if bg in text:\n",
        "            for i in range(0, len(tokens)-1):\n",
        "                bigram = tokens[i] + \" \" + tokens[i+1]\n",
        "                if (bg == bigram):\n",
        "                    count = count + 1\n",
        "        bi.append(count)\n",
        "    result.append(bi)\n",
        "    \n",
        "    # calculate the frequency of tri-gram\n",
        "    for tg in trigrams:\n",
        "        count = 0\n",
        "        if tg in text:\n",
        "            for i in range(0, len(tokens)-2):\n",
        "                trigram = tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2]\n",
        "                if (tg == trigram):\n",
        "                    count = count + 1\n",
        "        tri.append(count)\n",
        "    result.append(tri)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def lexicon_emoticon_features(text):\n",
        "    emotionvector = [0,0,0]\n",
        "    sent = text\n",
        "    sent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
        "    sent = sent.split()\n",
        "    \n",
        "    for word in text.split():\n",
        "        try:\n",
        "            if word in sent:\n",
        "                wordemotion = englexicon.loc[word.lower()]\n",
        "            else:\n",
        "                newword = to_hindi.convert([word])\n",
        "                newword = newword[0]\n",
        "                wordemotion = hinlexicon.loc[newword]\n",
        "  \n",
        "            emotionvector[0] = emotionvector[0] + wordemotion['Joy']\n",
        "            emotionvector[1] = emotionvector[1] + wordemotion['Sadness']\n",
        "            emotionvector[2] = emotionvector[2] + wordemotion['Anger']\n",
        "        \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return emotionvector"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDbvlveJQwhU",
        "outputId": "145cf844-77a8-48d6-e4fe-b2dd32a2ff91"
      },
      "source": [
        "for i in range(1,20):\n",
        "  txt = input(\"Enter the sentence : \")\n",
        "\n",
        "  # Vectorization of all features\n",
        "  features = []\n",
        "\n",
        "  features = emoji_features(txt)\n",
        "\n",
        "  features.append(punctuation_count(txt))\n",
        "\n",
        "  txt = text_processing(txt)\n",
        "\n",
        "  features.append(uppercase_feature(txt))\n",
        "  features.append(repeated_character_feature(txt))\n",
        "  features.append(negatedwords_feature(txt))\n",
        "  features = features + lexicon_emoticon_features(txt)\n",
        "  features.append(Intensifiers_feature(txt))\n",
        "  newlist = n_grams(txt)\n",
        "  features = features + newlist[0] + newlist[1] + newlist[2]\n",
        "\n",
        "  poly_pred = poly.predict([features])\n",
        "  print(poly_pred[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence : i am happy üòÇüòÇ\n",
            "Happiness\n",
            "Enter the sentence : i am crying\n",
            "Sadness\n",
            "Enter the sentence : i am crying üòÇüòÇ\n",
            "Happiness\n",
            "Enter the sentence : i am sad\n",
            "Sadness\n",
            "Enter the sentence : main udaas hu\n",
            "Sadness\n",
            "Enter the sentence :  he passed away\n",
            "Sadness\n",
            "Enter the sentence : he failed the exam\n",
            "Sadness\n",
            "Enter the sentence : he passed the exam\n",
            "Sadness\n",
            "Enter the sentence : STOP THIS PLISS !!\n",
            "Anger\n",
            "Enter the sentence : bann ipl üò†\n",
            "Anger\n",
            "Enter the sentence : ban ipl\n",
            "Sadness\n",
            "Enter the sentence : i wish you a very happy happy birthday\n",
            "Happiness\n",
            "Enter the sentence : i can't take this anymore\n",
            "Sadness\n",
            "Enter the sentence : I AM EXCITED\n",
            "Anger\n",
            "Enter the sentence : help me pls\n",
            "Sadness\n",
            "Enter the sentence : i am disappointed\n",
            "Sadness\n",
            "Enter the sentence : i am frustrated\n",
            "Sadness\n",
            "Enter the sentence : KAL MERA EXAM HAI i don't have anytime FOR NONSENSE\n",
            "Anger\n",
            "Enter the sentence : kyu mujhe DISTURB kar rahe ho\n",
            "Anger\n"
          ]
        }
      ]
    }
  ]
}